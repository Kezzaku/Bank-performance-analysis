# Analyse des Performances Bancaires (Berka Dataset)

## üìú Description du Projet

Ce projet vise √† analyser les performances financi√®res et op√©rationnelles d'une banque tch√®que fictive sur la p√©riode 1993-1998, √† partir du **Berka Dataset**. L'objectif est de fournir des insights exploitables aux parties prenantes (comme le CEO de la banque) via un tableau de bord interactif, en mettant en lumi√®re les tendances, les comportements des clients, et les risques financiers. Les donn√©es incluent des informations sur les clients (5369), les comptes (4500), les transactions (~1 million), les pr√™ts (~700), et les cartes de cr√©dit (~900).

Le pipeline de traitement des donn√©es extrait les donn√©es brutes, calcule les KPI (indicateurs cl√©s de performance), les stocke dans MongoDB Atlas, et les rend accessibles via un dashboard Next.js d√©ploy√© √† l'adresse suivante :  
[**Bank Performance Dashboard**](https://bank-performance.vercel.app/)

Le pipeline est automatis√© avec **Apache Airflow** pour garantir des mises √† jour r√©guli√®res des KPI.


## üìä KPI Calcul√©s et Int√©gr√©s dans le Dashboard

Les KPI suivants ont √©t√© calcul√©s et sont disponibles dans le dashboard :
- **`transaction_value_count_per_mounth_df.csv`** : Volume et chiffre d'affaires mensuels des transactions.
- **`transaction_operation.csv`** : R√©partition des types d'op√©rations (retraits, paiements, etc.).
- **`transaction_value_count_per_count_df.csv`** : Nombre et valeur moyenne des transactions par compte.
- **`dominant_district_count_df.csv`** : Districts avec le plus de transactions.
- **`dominant_district_loan_count.csv`** : Districts avec le plus de pr√™ts.
- **`loan_status_df.csv`** : R√©partition des statuts des pr√™ts (A, B, C, D).

Ces KPI permettent d‚Äôanalyser les tendances transactionnelles, les disparit√©s r√©gionales, les comportements des clients, et les risques li√©s aux pr√™ts.

---

## üõ†Ô∏è Architecture du Pipeline

Le pipeline est con√ßu pour traiter les donn√©es brutes, calculer les KPI, et les stocker dans une base de donn√©es MongoDB Atlas. Voici les √©tapes principales :

1. **Extraction des Donn√©es** :
   - Les donn√©es brutes (Berka Dataset) sont initialement stock√©es dans des fichiers CSV ou une base de donn√©es relationnelle (par exemple, Neon Postgres).
   - Les tables pertinentes (`trans`, `account`, `loan`, `district`, etc.) sont extraites.

2. **Calcul des KPI** :
   - Un script Python utilise `pandas` pour calculer les KPI mentionn√©s ci-dessus.
   - Les r√©sultats sont export√©s sous forme de fichiers CSV (par exemple, `transaction_value_count_per_mounth_df.csv`).

3. **Stockage dans MongoDB Atlas** :
   - Les KPI sont charg√©s dans MongoDB Atlas dans la base de donn√©es `payment_kpi_db`, avec une collection par KPI (par exemple, `volume_transactions`).
   - Un script Python utilise `pymongo` pour g√©rer la connexion et l‚Äôinsertion des donn√©es.

4. **Automatisation avec Apache Airflow** :
   - Airflow est utilis√© pour orchestrer le pipeline et automatiser les mises √† jour quotidiennes des KPI.
   - Un DAG (Directed Acyclic Graph) est d√©fini pour ex√©cuter les √©tapes d‚Äôextraction, calcul, et stockage.

5. **Dashboard Next.js** :
   - Le dashboard r√©cup√®re les KPI depuis MongoDB via des Server Actions Next.js, avec un fallback sur les fichiers CSV si la connexion √©choue.
   - Le dashboard est d√©ploy√© sur Vercel : [https://bank-performance.vercel.app/](https://bank-performance.vercel.app/).

---

## üñºÔ∏è Architecture Finale

![Architecture Finale du Pipeline](pipeline.png)

---

## üöÄ Mise en Place du Pipeline et Automatisation

### Pr√©requis

- **Python 3.12+** install√©.
- **MongoDB Atlas** : Un cluster configur√© avec une base de donn√©es `payment_kpi_db`.
- **Apache Airflow** : Install√© localement ou sur un serveur.
- **D√©pendances Python** : List√©es dans `requirements.txt`.

### √âtape 1 : Cloner le Projet

Clonez le d√©p√¥t contenant les scripts du pipeline :
```bash
git clone https://github.com/Martial2023/Bank-performance-pipeline.git
cd Bank-performance-pipeline
```

### √âtape 2 : Installer les D√©pendances

Installez les d√©pendances Python √† partir de `requirements.txt` :
```bash
pip install -r requirements.txt
```

Contenu de `requirements.txt` :
```
pandas==2.0.3
pymongo==4.6.1
apache-airflow==2.7.3
```

### √âtape 3 : Configurer les Variables d‚ÄôEnvironnement

Cr√©ez un fichier `.env` dans le r√©pertoire racine et ajoutez les variables suivantes :
```
MONGODB_URI=mongodb+srv://<username>:<password>@<cluster>.mongodb.net/payment_kpi_db?retryWrites=true&w=majority
DATA_PATH=/chemin/vers/berka-dataset/
```

- Remplacez `<username>`, `<password>`, et `<cluster>` par vos identifiants MongoDB Atlas.
- `DATA_PATH` doit pointer vers le r√©pertoire contenant les fichiers CSV bruts du Berka Dataset.

### √âtape 4 : Configurer Apache Airflow

1. **Initialiser Airflow** :
   Configurez Airflow et initialisez la base de donn√©es :
   ```bash
   export AIRFLOW_HOME=~/airflow
   airflow db init
   ```

2. **Cr√©er un Utilisateur Airflow** :
   ```bash
   airflow users create \
     --username admin \
     --firstname Admin \
     --lastname User \
     --role Admin \
     --email admin@example.com
   ```


### √âtape 5 : Lancer Airflow

1. **D√©marrer le Serveur Web Airflow** :
   ```bash
   airflow webserver --port 8080
   ```

2. **D√©marrer le Scheduler Airflow** (dans un autre terminal) :
   ```bash
   airflow scheduler
   ```

3. **Acc√©der √† l‚ÄôInterface Airflow** :
   - Ouvrez votre navigateur et allez √† `http://localhost:8080`.
   - Connectez-vous avec les identifiants cr√©√©s (par exemple, `admin`).
   - Activez le DAG `bank_pipeline` en cliquant sur le bouton "Toggle".

Le pipeline s‚Äôex√©cutera automatiquement tous les jours. Vous pouvez √©galement le d√©clencher manuellement via l‚Äôinterface Airflow.

---

## üìà Dashboard Final

Le tableau de bord interactif est d√©ploy√© sur Vercel :  
[**Bank Performance Dashboard**](https://bank-performance.vercel.app/)

Pour plus de d√©tails sur le lancement et l‚Äôutilisation du dashboard, consultez le d√©p√¥t GitHub d√©di√© :  
[**Bank Performance Dashboard Repository**](https://github.com/Martial2023/Bank-performance-analysis-dashboard)

---

## üìù Conclusion

Ce projet fournit une solution compl√®te pour analyser les performances d‚Äôune banque tch√®que √† partir du Berka Dataset. Le pipeline automatis√© avec Airflow garantit des KPI √† jour, tandis que le dashboard Next.js offre une interface intuitive pour explorer les donn√©es. Les KPI couvrent les transactions, les pr√™ts, et les disparit√©s r√©gionales, permettant d‚Äôidentifier les opportunit√©s (promotions saisonni√®res) et les risques (d√©fauts de pr√™t).

---

## ü§ù Contributions

Les contributions sont les bienvenues ! Si vous souhaitez ajouter de nouveaux KPI, am√©liorer le pipeline, ou optimiser le dashboard, ouvrez une issue ou soumettez une pull request sur les d√©p√¥ts GitHub.

---

## üìú Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.

---